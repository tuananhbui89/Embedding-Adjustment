{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_name = 'cs101'\n",
    "\n",
    "exp_gen_prompt = {\n",
    "    'carved_by': 'prompts_carved_by',\n",
    "    'inside': 'prompts_inside',\n",
    "    'painted_on': 'prompts_painted_on',\n",
    "}\n",
    "\n",
    "exp_names_ordered = [\n",
    "    'carved_by',\n",
    "    'inside', \n",
    "    'painted_on',\n",
    "]\n",
    "\n",
    "\n",
    "all_paths = {}\n",
    "all_paths['ReVersion'] = {\n",
    "    'custom_prompt_A': 'semantic_drift/output_{}/clip_alignment_scores_{}_{}_use_custom_prompt_objectA.csv',\n",
    "    'custom_prompt_B': 'semantic_drift/output_{}/clip_alignment_scores_{}_{}_use_custom_prompt_objectB.csv',\n",
    "    'full_prompt': 'semantic_drift/output_{}/clip_alignment_scores_{}_{}_use_custom_prompt_full_prompt.csv',\n",
    "}\n",
    "\n",
    "all_paths['ReVersion+TEA+0.2_0.5'] = {\n",
    "    'custom_prompt_A': 'semantic_drift/output_{}_tea_0.2_0.5/clip_alignment_scores_{}_tea_0.2_0.5_{}_use_custom_prompt_objectA.csv',\n",
    "    'custom_prompt_B': 'semantic_drift/output_{}_tea_0.2_0.5/clip_alignment_scores_{}_tea_0.2_0.5_{}_use_custom_prompt_objectB.csv',\n",
    "    'full_prompt': 'semantic_drift/output_{}_tea_0.2_0.5/clip_alignment_scores_{}_tea_0.2_0.5_{}_use_custom_prompt_full_prompt.csv',\n",
    "}\n",
    "\n",
    "all_paths['ReVersion+TEA+0.3_0.5'] = {\n",
    "    'custom_prompt_A': 'semantic_drift/output_{}_tea_0.3_0.5/clip_alignment_scores_{}_tea_0.3_0.5_{}_use_custom_prompt_objectA.csv',\n",
    "    'custom_prompt_B': 'semantic_drift/output_{}_tea_0.3_0.5/clip_alignment_scores_{}_tea_0.3_0.5_{}_use_custom_prompt_objectB.csv',\n",
    "    'full_prompt': 'semantic_drift/output_{}_tea_0.3_0.5/clip_alignment_scores_{}_tea_0.3_0.5_{}_use_custom_prompt_full_prompt.csv',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------carved_by ReVersion---------\n",
      "carved_by ReVersion custom_prompt_A 25.636471381187437\n",
      "carved_by ReVersion custom_prompt_B 15.923702639738716\n",
      "carved_by ReVersion full_prompt 27.73626562754313\n",
      "---------carved_by ReVersion+TEA+0.2_0.5---------\n",
      "carved_by ReVersion+TEA+0.2_0.5 custom_prompt_A 27.604278798103337\n",
      "carved_by ReVersion+TEA+0.2_0.5 custom_prompt_B 16.07649476687114\n",
      "carved_by ReVersion+TEA+0.2_0.5 full_prompt 29.924450917243956\n",
      "---------carved_by ReVersion+TEA+0.3_0.5---------\n",
      "carved_by ReVersion+TEA+0.3_0.5 custom_prompt_A 27.83533891995748\n",
      "carved_by ReVersion+TEA+0.3_0.5 custom_prompt_B 16.224426469802854\n",
      "carved_by ReVersion+TEA+0.3_0.5 full_prompt 30.166030356089276\n",
      "---------inside ReVersion---------\n",
      "inside ReVersion custom_prompt_A 24.968024450302124\n",
      "inside ReVersion custom_prompt_B 17.324630222320557\n",
      "inside ReVersion full_prompt 27.87094946098328\n",
      "---------inside ReVersion+TEA+0.2_0.5---------\n",
      "inside ReVersion+TEA+0.2_0.5 custom_prompt_A 25.21718749237061\n",
      "inside ReVersion+TEA+0.2_0.5 custom_prompt_B 17.644196110725407\n",
      "inside ReVersion+TEA+0.2_0.5 full_prompt 28.4241937828064\n",
      "---------inside ReVersion+TEA+0.3_0.5---------\n",
      "inside ReVersion+TEA+0.3_0.5 custom_prompt_A 25.154186710357667\n",
      "inside ReVersion+TEA+0.3_0.5 custom_prompt_B 17.526487386703486\n",
      "inside ReVersion+TEA+0.3_0.5 full_prompt 28.398779758453365\n",
      "---------painted_on ReVersion---------\n",
      "painted_on ReVersion custom_prompt_A 23.976645530700683\n",
      "painted_on ReVersion custom_prompt_B 20.12553572750091\n",
      "painted_on ReVersion full_prompt 30.074418994903567\n",
      "---------painted_on ReVersion+TEA+0.2_0.5---------\n",
      "painted_on ReVersion+TEA+0.2_0.5 custom_prompt_A 24.00437688446045\n",
      "painted_on ReVersion+TEA+0.2_0.5 custom_prompt_B 20.409372190475462\n",
      "painted_on ReVersion+TEA+0.2_0.5 full_prompt 30.122008613586427\n",
      "---------painted_on ReVersion+TEA+0.3_0.5---------\n",
      "painted_on ReVersion+TEA+0.3_0.5 custom_prompt_A 24.383631397247314\n",
      "painted_on ReVersion+TEA+0.3_0.5 custom_prompt_B 20.376535712242124\n",
      "painted_on ReVersion+TEA+0.3_0.5 full_prompt 30.349653648376467\n",
      "ReVersion  & carved_by  & 25.64/ 15.92/ 27.74/ -/ -    &  inside  & 24.97/ 17.32/ 27.87/ -/ -    &  painted_on  & 23.98/ 20.13/ 30.07/ -/ -    &  \n",
      "ReVersion+TEA+0.2_0.5  & carved_by  & 27.6/ 16.08/ 29.92/ -/ -    &  inside  & 25.22/ 17.64/ 28.42/ -/ -    &  painted_on  & 24.0/ 20.41/ 30.12/ -/ -    &  \n",
      "ReVersion+TEA+0.3_0.5  & carved_by  & 27.84/ 16.22/ 30.17/ -/ -    &  inside  & 25.15/ 17.53/ 28.4/ -/ -    &  painted_on  & 24.38/ 20.38/ 30.35/ -/ -    &  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "step = 1000\n",
    "\n",
    "all_results = dict()\n",
    "\n",
    "for exp_name in exp_names_ordered:\n",
    "    all_results[exp_name] = dict()\n",
    "    for method in all_paths.keys():\n",
    "        print(f'---------{exp_name} {method}---------')\n",
    "        all_results[exp_name][method] = dict()\n",
    "        for metric in all_paths[method].keys():\n",
    "            file_path = all_paths[method][metric].format(exp_name, exp_name, exp_gen_prompt[exp_name])\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"File {file_path} does not exist\")\n",
    "                continue\n",
    "            # else:\n",
    "            #     print(f\"** File {file_path} exists\")\n",
    "            df = pd.read_csv(file_path)\n",
    "            results = dict()\n",
    "            temp = []\n",
    "            for prompt_index in list(df['prompt_index'].unique()):\n",
    "                prompt_df = df[df['prompt_index'] == prompt_index]\n",
    "                results[prompt_index] = {\n",
    "                    'prompt': prompt_df['prompt'].iloc[0],\n",
    "                    'mean': prompt_df['score'].mean(),\n",
    "                    'std': prompt_df['score'].std(),\n",
    "                    'min': prompt_df['score'].min(),\n",
    "                    'max': prompt_df['score'].max(),\n",
    "                    'median': prompt_df['score'].median()\n",
    "                }\n",
    "                temp.append(prompt_df['score'].mean())\n",
    "            all_results[exp_name][method][metric] = results\n",
    "            # calculate the average over all prompts\n",
    "            print(exp_name, method, metric, np.mean(temp))\n",
    "\n",
    "\n",
    "metrics_list = ['custom_prompt_A', 'custom_prompt_B', 'full_prompt', 'vs_anchor_t01', 'dino_t01']\n",
    "# metrics_list = ['full_prompt', 'vs_anchor_t01']\n",
    "\n",
    "\n",
    "for method in all_paths.keys():\n",
    "    write_str = f\"{method}  & \"\n",
    "    for exp_name in all_results.keys():\n",
    "        write_str += f\"{exp_name}  & \"\n",
    "        if method in all_results[exp_name].keys():\n",
    "            temp_str = \"\"\n",
    "            for im, metric in enumerate(metrics_list):\n",
    "                if metric in all_results[exp_name][method].keys():\n",
    "                    temp = all_results[exp_name][method][metric]\n",
    "                    if metric == 'dino_t01':\n",
    "                        scale = 100\n",
    "                    else:\n",
    "                        scale = 1\n",
    "                    temp = [temp[i]['mean'] * scale for i in temp.keys()]\n",
    "                    avg = np.round(np.mean(temp), 2)\n",
    "                    A = str(avg)\n",
    "                else:\n",
    "                    A = \"-\"\n",
    "                if im == len(metrics_list) - 1:\n",
    "                    temp_str += f\"{A}  \"\n",
    "                else:\n",
    "                    temp_str += f\"{A}/ \"\n",
    "            write_str += f\"{temp_str}  &  \"\n",
    "        else:\n",
    "            temp_str = \"\"\n",
    "            for im, metric in enumerate(metrics_list):\n",
    "                if im == len(metrics_list) - 1:\n",
    "                    temp_str += \"- \"\n",
    "                else:\n",
    "                    temp_str += \"-/ \"\n",
    "            write_str += f\"{temp_str}  &  \"\n",
    "    print(write_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting_name cs101 Gap between EasyControl EasyControl+TEA+0.2_0.5 custom_prompt_A 0.08000000000000007\n",
      "setting_name cs101 Gap between EasyControl EasyControl+TEA+0.2_0.5 custom_prompt_B 0.009999999999999787\n",
      "setting_name cs101 Gap between EasyControl EasyControl+TEA+0.2_0.5 full_prompt 0.254999999999999\n",
      "setting_name cs101 Gap between EasyControl EasyControl+TEA+0.2_0.5 vs_anchor_t01 1.6149999999999984\n",
      "setting_name cs101 Gap between EasyControl EasyControl+TEA+0.2_0.5 dino_t01 2.030000000000001\n"
     ]
    }
   ],
   "source": [
    "collect_results = {}\n",
    "for method in all_paths.keys():\n",
    "    write_str = f\"{method}&  \"\n",
    "    collect_results[method] = {}\n",
    "    for metric in metrics_list:\n",
    "        collect_results[method][metric] = []\n",
    "\n",
    "    for exp_name in all_results.keys():\n",
    "\n",
    "        if method in all_results[exp_name].keys():\n",
    "\n",
    "            for metric in metrics_list:\n",
    "\n",
    "                if metric in all_results[exp_name][method].keys():\n",
    "                    temp = all_results[exp_name][method][metric]\n",
    "                    if metric == 'dino_t01':\n",
    "                        scale = 100\n",
    "                    else:\n",
    "                        scale = 1   \n",
    "                    temp = [temp[i]['mean'] * scale for i in temp.keys()]\n",
    "                    avg = np.round(np.mean(temp), 2)\n",
    "                    A = avg\n",
    "                else:\n",
    "                    A = \"-\"\n",
    "\n",
    "                collect_results[method][metric].append(A)\n",
    "        else:\n",
    "\n",
    "            for metric in metrics_list:\n",
    "                collect_results[method][metric].append(\"-\")\n",
    "\n",
    "method1s = [\"EasyControl\"]\n",
    "method2s = [\"EasyControl+TEA+0.2_0.5\"]\n",
    "\n",
    "for method1, method2 in zip(method1s, method2s):\n",
    "    for metric in metrics_list:\n",
    "        values1 = collect_results[method1][metric]\n",
    "        values2 = collect_results[method2][metric]\n",
    "        differences = []\n",
    "            \n",
    "        for v1, v2 in zip(values1, values2):\n",
    "            # Skip if either value is not numeric\n",
    "            if v1 == \"-\" or v2 == \"-\":\n",
    "                continue\n",
    "                \n",
    "            # Calculate difference and add to list\n",
    "            diff = float(v2) - float(v1)\n",
    "            differences.append(diff)\n",
    "        if differences:\n",
    "            print('setting_name', setting_name,'Gap between', method1, method2, metric, sum(differences) / len(differences))\n",
    "        else:\n",
    "            print(\"No valid pairs for comparison\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
